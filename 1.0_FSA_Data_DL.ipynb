{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a7f510-913e-4d9c-a724-434cd1985b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from io import BytesIO\n",
    "from concurrent.futures import ProcessPoolExecutor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b55f0-63ea-4a65-8045-584d3d4955e8",
   "metadata": {},
   "source": [
    "We perform a web scraping to extract links to Excel files (.xlsx) from FSA payment webpage. The script identifies all the <a> tags (hyperlinks) within the webpage and filters out only those links that point to Excel files. After extracting the desired links, it constructs the absolute URLs by appending them to the main URL of the FSA website. For each Excel file URL, it retrieves the file using urllib.request.urlretrieve() and saves it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ebed24-9a22-42e4-ac13-b64352815e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.fsa.usda.gov/news-room/efoia/electronic-reading-room/frequently-requested-information/payment-files-information/index')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b85a952-58fe-4ac2-9d67-9e5fd91ba403",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9196611-aa05-40c7-a255-f4141e2e14cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text,'html.parser')\n",
    "#soup\n",
    "\n",
    "links = soup.find_all(\"a\")\n",
    "#links[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ecf86-6af0-43bf-b713-a0723301e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_links = [link.get('href') for link in links if link.get('href') and link.get('href').endswith('.xlsx')]\n",
    "main_url = \"https://www.fsa.usda.gov\"\n",
    "\n",
    "for new_url in xlsx_links:\n",
    "    dls = f\"{main_url}{new_url}\"\n",
    "    filename = new_url.split('/')[-1]  # Extract the filename from the URL\n",
    "    urllib.request.urlretrieve(dls, filename)\n",
    "    print(f\"Downloaded {filename}\")\n",
    "\n",
    "print(\"Download completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf06dbc-a8fc-440c-92d8-2b4363ee91d9",
   "metadata": {},
   "source": [
    "Next, we combined multiple Excel files into a single CSV file. It begins by importing the necessary libraries, pandas and os. \n",
    "It defines a directory where the Excel files are located and lists all files with .xlsx or .xls extensions in that directory. Then, it creates an empty list to store pandas DataFrames, iterates through each Excel file in the directory, reads each file into a DataFrame using pd.read_excel(), and appends the DataFrame to the list. After reading all Excel files, it concatenates all DataFrames in the list into a single DataFrame using pd.concat(). The parameter ignore_index=True ensures that the index of the resulting DataFrame is reset. Finally, it saves the combined DataFrame to a CSV file named 'combined_data.csv' using the to_csv() method, specifying index=False to exclude the index column from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983ca38-91b7-4f64-800c-64439d639999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "directory = r'D:/OneDrive - University of Illinois - Urbana/JupNote/FSAExcels/'\n",
    "excel_files = [f for f in os.listdir(directory) if f.endswith('.xlsx') or f.endswith('.xls')]\n",
    "\n",
    "dffs = []\n",
    "for file in excel_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    dff = pd.read_excel(file_path)\n",
    "    dffs.append(dff)\n",
    "\n",
    "combined_df = pd.concat(dffs, ignore_index=True)\n",
    "combined_df.to_csv('combined_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5ebdda-12a5-4128-9a01-5fb6eb964f53",
   "metadata": {},
   "source": [
    "Here code reads in dataset in chunks and creates a list of Conservation Reserve Programs (CRP). Takes the unique content of 'Accounting Program Description' that has 'CRP' in their program name in each chunk and updates the list for the whole dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2434e-5338-4b47-bb24-dc4291d3e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "unique_programs = set()\n",
    "chonker = pd.read_csv('D:/OneDrive - University of Illinois - Urbana/JupNote/combined_data.csv', chunksize=100000, low_memory=False)\n",
    "\n",
    "\n",
    "for i in chonker:\n",
    "    unique_programs.update(i[i['Accounting Program Description'].str.contains('CRP ', na=False)]['Accounting Program Description'].unique())\n",
    "\n",
    "unique_programs_list = list(unique_programs)\n",
    "unique_programs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82588f9b-094e-43e7-8360-acbb991839eb",
   "metadata": {},
   "source": [
    "Here the code creates a seperate csv file only for CRP programs using the list created in the previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c8a271-f90f-4562-a36e-0241af0ba4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the CSV file\n",
    "file_path = 'D:/OneDrive - University of Illinois - Urbana/JupNote/combined_data.csv'\n",
    "# Output CSV file\n",
    "output_file = 'D:/OneDrive - University of Illinois - Urbana/JupNote/CRP_data.csv'\n",
    "\n",
    "# Initialize chunk reader\n",
    "chunk_iter = pd.read_csv(file_path, chunksize=1000000, low_memory=False)\n",
    "\n",
    "# Initialize a flag to check if headers need to be written\n",
    "write_header = not os.path.exists(output_file)  # True if file does not exist\n",
    "\n",
    "# Loop through each chunk\n",
    "for chunk in chunk_iter:\n",
    "    # Filter the chunk\n",
    "    chunk['Payment Date'] = pd.to_datetime(chunk['Payment Date'], errors='coerce')\n",
    "    chunk['Payment Year'] = chunk['Payment Date'].dt.year\n",
    "    filtered_chunk = chunk[chunk['Accounting Program Description'].isin(unique_programs_list)]\n",
    "\n",
    "    # Check if filtered_chunk is not empty\n",
    "    if not filtered_chunk.empty:\n",
    "        # Append the filtered chunk to the output CSV file\n",
    "        # Write header only if write_header is True\n",
    "        filtered_chunk.to_csv(output_file, mode='a', header=write_header, index=False)\n",
    "        write_header = False  # Set to False after first write\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
